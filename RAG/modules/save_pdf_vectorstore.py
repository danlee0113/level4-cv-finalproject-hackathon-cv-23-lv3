from pdf import process_pdfs_in_directory
from embeddings import recursive_embed_cluster_summarize, load_json
from langchain_community.vectorstores import FAISS

import time 

PDF_DIR="/data/ephemeral/home/dataset/"
DB_INDEX = "RAPTOR_CLAUDE_1" 
pdf_directory = PDF_DIR # ë°ì´í„° ìœ„ì¹˜ 
LLM_INFO_DIR="/data/ephemeral/home/level4-cv-finalproject-hackathon-cv-23-lv3/RAG/json_cache/llm_info.json"
EMBEDDING_INFO_DIR="/data/ephemeral/home/level4-cv-finalproject-hackathon-cv-23-lv3/RAG/json_cache/embeddings_info.json"
BATCH_SIZE= 100


embeddings, llm=load_json(EMBEDDING_INFO_DIR, LLM_INFO_DIR)



# PDF ì²˜ë¦¬ ë° ì²­í¬ ìƒì„±
texts_split = process_pdfs_in_directory(pdf_directory, chunk_size=4000, chunk_overlap=500)

# íŠ¸ë¦¬ êµ¬ì¶• (main)
leaf_texts = texts_split.copy()

# ì¬ê·€ì ìœ¼ë¡œ ì„ë² ë”©, í´ëŸ¬ìŠ¤í„°ë§ ë° ìš”ì•½ì„ ìˆ˜í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ì–»ìŒ
results = recursive_embed_cluster_summarize(leaf_texts, level=1, n_levels=3)

# DB, VectorStore

all_texts = leaf_texts.copy()

# ë ˆë²¨ì„ ì •ë ¬í•˜ì—¬ ìˆœíšŒ
for level in sorted(results.keys()):
    # í˜„ì¬ ë ˆë²¨ì˜ DataFrameì—ì„œ ìš”ì•½ì„ ì¶”ì¶œ
    summaries = results[level][1]["summaries"].tolist()
    # í˜„ì¬ ë ˆë²¨ì˜ ìš”ì•½ì„ all_textsì— ì¶”ê°€í•©ë‹ˆë‹¤.
    all_texts.extend(summaries)

# ì´ì œ all_textsë¥¼ ì‚¬ìš©í•˜ì—¬ FAISS vectorstoreë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.


# 1ï¸âƒ£ FAISS ì´ˆê¸°í™” (ì²« ë²ˆì§¸ ë°°ì¹˜ë¥¼ ì´ìš©)
first_batch_texts = all_texts[:BATCH_SIZE]
first_batch_embeddings = embeddings.embed_documents(first_batch_texts)
vectorstore = FAISS.from_texts(texts=first_batch_texts, embedding=embeddings)

# 2ï¸âƒ£ ë‚˜ë¨¸ì§€ ë°ì´í„° ë°°ì¹˜ë¡œ ì¶”ê°€
for i in range(BATCH_SIZE, len(all_texts), BATCH_SIZE):
    batch_texts = all_texts[i:i + BATCH_SIZE]
    batch_embeddings = embeddings.embed_documents(batch_texts)
    vectorstore.add_texts(texts=batch_texts, embeddings=batch_embeddings)
    time.sleep(1)  # ğŸŒŸ API Rate Limitì„ í”¼í•˜ê¸° ìœ„í•´ 1ì´ˆ ëŒ€ê¸°

# 3ï¸âƒ£ ë²¡í„°ìŠ¤í† ì–´ ì €ì¥
vectorstore.save_local(folder_path=DB_INDEX)